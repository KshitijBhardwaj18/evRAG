"""
Evaluation Runner: Orchestrates the entire evaluation process

Runs retrieval, generation, and hallucination metrics for a single query.
"""
from typing import List, Any, Dict, Optional
from .retrieval import (
    calculate_recall_at_k,
    calculate_precision_at_k,
    calculate_mrr,
    calculate_map,
    calculate_hit_rate,
    calculate_coverage
)
from .generation import (
    calculate_faithfulness,
    calculate_answer_relevance,
    calculate_context_utilization,
    calculate_semantic_similarity,
    calculate_rouge_l,
    calculate_f1_score
)
from .hallucination import (
    detect_hallucination_llm,
    check_citations,
    calculate_embedding_drift,
    aggregate_hallucination_score
)
from ..core.logging import log
from ..core.config import settings


class EvaluationRunner:
    """Orchestrates evaluation for RAG systems"""
    
    def __init__(self, embedding_model=None, openai_api_key: Optional[str] = None):
        """
        Initialize evaluation runner.
        
        Args:
            embedding_model: SentenceTransformer model for embeddings
            openai_api_key: OpenAI API key for LLM judge
        """
        self.embedding_model = embedding_model
        self.openai_api_key = openai_api_key or settings.OPENAI_API_KEY
        
    def evaluate_single(
        self,
        query: str,
        retrieved_docs: List[Any],
        generated_answer: str,
        ground_truth_docs: Optional[List[Any]] = None,
        ground_truth_answer: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Evaluate a single query-answer pair.
        
        Args:
            query: The user query
            retrieved_docs: Documents retrieved by RAG system
            generated_answer: Answer generated by RAG system
            ground_truth_docs: Ground truth relevant documents
            ground_truth_answer: Ground truth answer (optional)
            
        Returns:
            Dictionary with all evaluation metrics
        """
        log.info(f"Evaluating query: {query[:50]}...")
        
        results = {
            "query": query,
            "retrieved_docs": retrieved_docs,
            "generated_answer": generated_answer
        }
        
        # === RETRIEVAL METRICS ===
        if ground_truth_docs:
            try:
                results["recall_at_k"] = calculate_recall_at_k(retrieved_docs, ground_truth_docs)
                results["precision_at_k"] = calculate_precision_at_k(retrieved_docs, ground_truth_docs)
                results["mrr"] = calculate_mrr(retrieved_docs, ground_truth_docs)
                results["map_score"] = calculate_map(retrieved_docs, ground_truth_docs)
                results["hit_rate"] = calculate_hit_rate(retrieved_docs, ground_truth_docs)
                results["coverage"] = calculate_coverage(retrieved_docs, ground_truth_docs)
                log.debug("Retrieval metrics calculated")
            except Exception as e:
                log.error(f"Error calculating retrieval metrics: {e}")
                results["retrieval_error"] = str(e)
        else:
            log.warning("No ground truth docs provided, skipping retrieval metrics")
        
        # === GENERATION METRICS ===
        try:
            # Faithfulness
            faithfulness_result = calculate_faithfulness(
                generated_answer, retrieved_docs, self.embedding_model
            )
            results["faithfulness"] = faithfulness_result["score"]
            results["faithfulness_detail"] = faithfulness_result
            
            # Answer relevance
            relevance_result = calculate_answer_relevance(
                query, generated_answer, self.embedding_model
            )
            results["answer_relevance"] = relevance_result["score"]
            results["answer_relevance_detail"] = relevance_result
            
            # Context utilization
            utilization_result = calculate_context_utilization(
                generated_answer, retrieved_docs, self.embedding_model
            )
            results["context_utilization"] = utilization_result["score"]
            results["context_utilization_detail"] = utilization_result
            
            log.debug("Generation metrics calculated")
        except Exception as e:
            log.error(f"Error calculating generation metrics: {e}")
            results["generation_error"] = str(e)
        
        # === SEMANTIC SIMILARITY (if ground truth answer exists) ===
        if ground_truth_answer:
            try:
                sim_result = calculate_semantic_similarity(
                    generated_answer, ground_truth_answer, self.embedding_model
                )
                results["semantic_similarity"] = sim_result["score"]
                results["semantic_similarity_detail"] = sim_result
                
                # ROUGE-L and F1
                results["rouge_l"] = calculate_rouge_l(generated_answer, ground_truth_answer)
                results["f1_score"] = calculate_f1_score(generated_answer, ground_truth_answer)
                
                log.debug("Semantic similarity metrics calculated")
            except Exception as e:
                log.error(f"Error calculating similarity metrics: {e}")
                results["similarity_error"] = str(e)
        
        # === HALLUCINATION DETECTION ===
        try:
            # LLM Judge
            llm_judge = detect_hallucination_llm(
                generated_answer, retrieved_docs, self.openai_api_key
            )
            
            # Citation Check
            citation_check = check_citations(generated_answer, retrieved_docs)
            results["citation_coverage"] = citation_check["citation_coverage"]
            
            # Embedding Drift
            drift = calculate_embedding_drift(
                generated_answer, retrieved_docs, self.embedding_model
            )
            
            # Aggregate hallucination score
            hallucination_aggregate = aggregate_hallucination_score(
                llm_judge, citation_check, drift
            )
            
            results["hallucination_score"] = hallucination_aggregate["hallucination_score"]
            results["hallucinated_spans"] = hallucination_aggregate["hallucinated_spans"]
            results["hallucination_detail"] = hallucination_aggregate
            
            log.debug("Hallucination detection completed")
        except Exception as e:
            log.error(f"Error in hallucination detection: {e}")
            results["hallucination_error"] = str(e)
        
        log.info("Evaluation completed successfully")
        return results
    
    def calculate_aggregate_metrics(self, results_list: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Calculate aggregate metrics across multiple evaluations.
        
        Args:
            results_list: List of individual evaluation results
            
        Returns:
            Dictionary with aggregated metrics
        """
        if not results_list:
            return {}
        
        aggregates = {}
        
        # Define metrics to aggregate
        numeric_metrics = [
            "mrr", "map_score", "hit_rate", "coverage",
            "faithfulness", "answer_relevance", "context_utilization",
            "semantic_similarity", "rouge_l", "f1_score",
            "hallucination_score", "citation_coverage"
        ]
        
        # Recall@K and Precision@K need special handling
        k_values = [1, 3, 5, 10]
        for k in k_values:
            recall_vals = [r.get("recall_at_k", {}).get(k) for r in results_list if r.get("recall_at_k")]
            recall_vals = [v for v in recall_vals if v is not None]
            if recall_vals:
                aggregates[f"avg_recall_at_{k}"] = round(sum(recall_vals) / len(recall_vals), 4)
            
            precision_vals = [r.get("precision_at_k", {}).get(k) for r in results_list if r.get("precision_at_k")]
            precision_vals = [v for v in precision_vals if v is not None]
            if precision_vals:
                aggregates[f"avg_precision_at_{k}"] = round(sum(precision_vals) / len(precision_vals), 4)
        
        # Average other metrics
        for metric in numeric_metrics:
            values = [r.get(metric) for r in results_list if r.get(metric) is not None]
            if values:
                aggregates[f"avg_{metric}"] = round(sum(values) / len(values), 4)
                aggregates[f"min_{metric}"] = round(min(values), 4)
                aggregates[f"max_{metric}"] = round(max(values), 4)
        
        # Count queries with hallucinations
        hallucination_count = sum(
            1 for r in results_list 
            if r.get("hallucination_score", 0) > 0.5
        )
        aggregates["queries_with_hallucinations"] = hallucination_count
        aggregates["hallucination_rate"] = round(hallucination_count / len(results_list), 4)
        
        aggregates["total_queries"] = len(results_list)
        
        return aggregates


def load_embedding_model(model_name: Optional[str] = None):
    """
    Load sentence transformer embedding model.
    
    Args:
        model_name: Name of the model to load
        
    Returns:
        Loaded SentenceTransformer model or None
    """
    try:
        from sentence_transformers import SentenceTransformer
        
        model_name = model_name or settings.DEFAULT_EMBEDDING_MODEL
        log.info(f"Loading embedding model: {model_name}")
        
        model = SentenceTransformer(model_name)
        log.info("Embedding model loaded successfully")
        return model
    except Exception as e:
        log.error(f"Failed to load embedding model: {e}")
        return None

